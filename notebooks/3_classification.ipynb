{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook 3 — Churn Classification\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (roc_auc_score, average_precision_score,\n",
    "                             classification_report, confusion_matrix)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pickle\n",
    "\n",
    "RSEED = 42\n",
    "np.random.seed(RSEED)\n",
    "pd.set_option(\"display.max_columns\", 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged shape: (5901, 46)\n",
      "Churn balance: {0: 0.75, 1: 0.25}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>first_purchase</th>\n",
       "      <th>last_purchase</th>\n",
       "      <th>total_orders</th>\n",
       "      <th>total_quantity</th>\n",
       "      <th>total_revenue</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>region</th>\n",
       "      <th>segment</th>\n",
       "      <th>tenure_months</th>\n",
       "      <th>unique_products</th>\n",
       "      <th>unique_categories</th>\n",
       "      <th>sentiment_mean</th>\n",
       "      <th>sent_share_negative_x</th>\n",
       "      <th>sent_share_neutral_x</th>\n",
       "      <th>sent_share_positive_x</th>\n",
       "      <th>recency_days</th>\n",
       "      <th>aov</th>\n",
       "      <th>orders_per_month</th>\n",
       "      <th>rev_90d</th>\n",
       "      <th>rev_90d_share</th>\n",
       "      <th>level_1</th>\n",
       "      <th>0</th>\n",
       "      <th>recency</th>\n",
       "      <th>frequency</th>\n",
       "      <th>monetary</th>\n",
       "      <th>recency_scaled</th>\n",
       "      <th>frequency_scaled</th>\n",
       "      <th>monetary_scaled</th>\n",
       "      <th>cluster</th>\n",
       "      <th>segment_label</th>\n",
       "      <th>compound_mean</th>\n",
       "      <th>compound_median</th>\n",
       "      <th>compound_min</th>\n",
       "      <th>compound_max</th>\n",
       "      <th>neg_mean</th>\n",
       "      <th>neu_mean</th>\n",
       "      <th>pos_mean</th>\n",
       "      <th>sentiment_last</th>\n",
       "      <th>sentiment_vader_last</th>\n",
       "      <th>compound_last</th>\n",
       "      <th>sent_share_negative_y</th>\n",
       "      <th>sent_share_neutral_y</th>\n",
       "      <th>sent_share_positive_y</th>\n",
       "      <th>churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CUST00001</td>\n",
       "      <td>2023-05-19</td>\n",
       "      <td>2023-07-18</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>249000</td>\n",
       "      <td>43</td>\n",
       "      <td>Female</td>\n",
       "      <td>South</td>\n",
       "      <td>Corporate</td>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>166</td>\n",
       "      <td>124500.0</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ip_gap_mean</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>166</td>\n",
       "      <td>2</td>\n",
       "      <td>187000.0</td>\n",
       "      <td>-0.178873</td>\n",
       "      <td>-1.085640</td>\n",
       "      <td>-0.077402</td>\n",
       "      <td>0</td>\n",
       "      <td>Segment 2</td>\n",
       "      <td>0.285950</td>\n",
       "      <td>0.28595</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5719</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.67550</td>\n",
       "      <td>0.3245</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.5719</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CUST00001</td>\n",
       "      <td>2023-05-19</td>\n",
       "      <td>2023-07-18</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>249000</td>\n",
       "      <td>43</td>\n",
       "      <td>Female</td>\n",
       "      <td>South</td>\n",
       "      <td>Corporate</td>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>166</td>\n",
       "      <td>124500.0</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ip_gap_median</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>166</td>\n",
       "      <td>2</td>\n",
       "      <td>187000.0</td>\n",
       "      <td>-0.178873</td>\n",
       "      <td>-1.085640</td>\n",
       "      <td>-0.077402</td>\n",
       "      <td>0</td>\n",
       "      <td>Segment 2</td>\n",
       "      <td>0.285950</td>\n",
       "      <td>0.28595</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5719</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.67550</td>\n",
       "      <td>0.3245</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.5719</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CUST00001</td>\n",
       "      <td>2023-05-19</td>\n",
       "      <td>2023-07-18</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>249000</td>\n",
       "      <td>43</td>\n",
       "      <td>Female</td>\n",
       "      <td>South</td>\n",
       "      <td>Corporate</td>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>166</td>\n",
       "      <td>124500.0</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ip_gap_std</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>166</td>\n",
       "      <td>2</td>\n",
       "      <td>187000.0</td>\n",
       "      <td>-0.178873</td>\n",
       "      <td>-1.085640</td>\n",
       "      <td>-0.077402</td>\n",
       "      <td>0</td>\n",
       "      <td>Segment 2</td>\n",
       "      <td>0.285950</td>\n",
       "      <td>0.28595</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5719</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.67550</td>\n",
       "      <td>0.3245</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.5719</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CUST00002</td>\n",
       "      <td>2022-01-19</td>\n",
       "      <td>2023-06-27</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>84300</td>\n",
       "      <td>22</td>\n",
       "      <td>Male</td>\n",
       "      <td>East</td>\n",
       "      <td>Corporate</td>\n",
       "      <td>37</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "      <td>187</td>\n",
       "      <td>21075.0</td>\n",
       "      <td>0.108108</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ip_gap_mean</td>\n",
       "      <td>146.333333</td>\n",
       "      <td>187</td>\n",
       "      <td>4</td>\n",
       "      <td>84300.0</td>\n",
       "      <td>0.023153</td>\n",
       "      <td>-0.035245</td>\n",
       "      <td>-0.807727</td>\n",
       "      <td>0</td>\n",
       "      <td>Segment 2</td>\n",
       "      <td>0.051050</td>\n",
       "      <td>-0.00905</td>\n",
       "      <td>-0.4585</td>\n",
       "      <td>0.6808</td>\n",
       "      <td>0.21400</td>\n",
       "      <td>0.54900</td>\n",
       "      <td>0.2370</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Negative</td>\n",
       "      <td>-0.4585</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CUST00002</td>\n",
       "      <td>2022-01-19</td>\n",
       "      <td>2023-06-27</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>84300</td>\n",
       "      <td>22</td>\n",
       "      <td>Male</td>\n",
       "      <td>East</td>\n",
       "      <td>Corporate</td>\n",
       "      <td>37</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "      <td>187</td>\n",
       "      <td>21075.0</td>\n",
       "      <td>0.108108</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ip_gap_median</td>\n",
       "      <td>138.000000</td>\n",
       "      <td>187</td>\n",
       "      <td>4</td>\n",
       "      <td>84300.0</td>\n",
       "      <td>0.023153</td>\n",
       "      <td>-0.035245</td>\n",
       "      <td>-0.807727</td>\n",
       "      <td>0</td>\n",
       "      <td>Segment 2</td>\n",
       "      <td>0.051050</td>\n",
       "      <td>-0.00905</td>\n",
       "      <td>-0.4585</td>\n",
       "      <td>0.6808</td>\n",
       "      <td>0.21400</td>\n",
       "      <td>0.54900</td>\n",
       "      <td>0.2370</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Negative</td>\n",
       "      <td>-0.4585</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CUST00002</td>\n",
       "      <td>2022-01-19</td>\n",
       "      <td>2023-06-27</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>84300</td>\n",
       "      <td>22</td>\n",
       "      <td>Male</td>\n",
       "      <td>East</td>\n",
       "      <td>Corporate</td>\n",
       "      <td>37</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "      <td>187</td>\n",
       "      <td>21075.0</td>\n",
       "      <td>0.108108</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ip_gap_std</td>\n",
       "      <td>110.735420</td>\n",
       "      <td>187</td>\n",
       "      <td>4</td>\n",
       "      <td>84300.0</td>\n",
       "      <td>0.023153</td>\n",
       "      <td>-0.035245</td>\n",
       "      <td>-0.807727</td>\n",
       "      <td>0</td>\n",
       "      <td>Segment 2</td>\n",
       "      <td>0.051050</td>\n",
       "      <td>-0.00905</td>\n",
       "      <td>-0.4585</td>\n",
       "      <td>0.6808</td>\n",
       "      <td>0.21400</td>\n",
       "      <td>0.54900</td>\n",
       "      <td>0.2370</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Negative</td>\n",
       "      <td>-0.4585</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CUST00003</td>\n",
       "      <td>2020-07-18</td>\n",
       "      <td>2023-02-06</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>130150</td>\n",
       "      <td>59</td>\n",
       "      <td>Female</td>\n",
       "      <td>North</td>\n",
       "      <td>Corporate</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>328</td>\n",
       "      <td>32537.5</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ip_gap_mean</td>\n",
       "      <td>339.666667</td>\n",
       "      <td>328</td>\n",
       "      <td>4</td>\n",
       "      <td>130150.0</td>\n",
       "      <td>1.379618</td>\n",
       "      <td>-0.035245</td>\n",
       "      <td>-0.481676</td>\n",
       "      <td>0</td>\n",
       "      <td>Segment 2</td>\n",
       "      <td>0.276875</td>\n",
       "      <td>0.50615</td>\n",
       "      <td>-0.4767</td>\n",
       "      <td>0.5719</td>\n",
       "      <td>0.09575</td>\n",
       "      <td>0.47475</td>\n",
       "      <td>0.4295</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.5719</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CUST00003</td>\n",
       "      <td>2020-07-18</td>\n",
       "      <td>2023-02-06</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>130150</td>\n",
       "      <td>59</td>\n",
       "      <td>Female</td>\n",
       "      <td>North</td>\n",
       "      <td>Corporate</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>328</td>\n",
       "      <td>32537.5</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ip_gap_median</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>328</td>\n",
       "      <td>4</td>\n",
       "      <td>130150.0</td>\n",
       "      <td>1.379618</td>\n",
       "      <td>-0.035245</td>\n",
       "      <td>-0.481676</td>\n",
       "      <td>0</td>\n",
       "      <td>Segment 2</td>\n",
       "      <td>0.276875</td>\n",
       "      <td>0.50615</td>\n",
       "      <td>-0.4767</td>\n",
       "      <td>0.5719</td>\n",
       "      <td>0.09575</td>\n",
       "      <td>0.47475</td>\n",
       "      <td>0.4295</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.5719</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CUST00003</td>\n",
       "      <td>2020-07-18</td>\n",
       "      <td>2023-02-06</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>130150</td>\n",
       "      <td>59</td>\n",
       "      <td>Female</td>\n",
       "      <td>North</td>\n",
       "      <td>Corporate</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>328</td>\n",
       "      <td>32537.5</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ip_gap_std</td>\n",
       "      <td>126.040999</td>\n",
       "      <td>328</td>\n",
       "      <td>4</td>\n",
       "      <td>130150.0</td>\n",
       "      <td>1.379618</td>\n",
       "      <td>-0.035245</td>\n",
       "      <td>-0.481676</td>\n",
       "      <td>0</td>\n",
       "      <td>Segment 2</td>\n",
       "      <td>0.276875</td>\n",
       "      <td>0.50615</td>\n",
       "      <td>-0.4767</td>\n",
       "      <td>0.5719</td>\n",
       "      <td>0.09575</td>\n",
       "      <td>0.47475</td>\n",
       "      <td>0.4295</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.5719</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CUST00004</td>\n",
       "      <td>2020-03-26</td>\n",
       "      <td>2023-11-28</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>349000</td>\n",
       "      <td>29</td>\n",
       "      <td>Male</td>\n",
       "      <td>East</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.60</td>\n",
       "      <td>33</td>\n",
       "      <td>69800.0</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ip_gap_mean</td>\n",
       "      <td>230.000000</td>\n",
       "      <td>33</td>\n",
       "      <td>5</td>\n",
       "      <td>302000.0</td>\n",
       "      <td>-1.458376</td>\n",
       "      <td>0.489953</td>\n",
       "      <td>0.740391</td>\n",
       "      <td>1</td>\n",
       "      <td>Segment 1</td>\n",
       "      <td>0.308720</td>\n",
       "      <td>0.42010</td>\n",
       "      <td>-0.3089</td>\n",
       "      <td>0.5719</td>\n",
       "      <td>0.08580</td>\n",
       "      <td>0.50760</td>\n",
       "      <td>0.4066</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.4201</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  customer_id first_purchase last_purchase  total_orders  total_quantity  \\\n",
       "0   CUST00001     2023-05-19    2023-07-18             2               7   \n",
       "1   CUST00001     2023-05-19    2023-07-18             2               7   \n",
       "2   CUST00001     2023-05-19    2023-07-18             2               7   \n",
       "3   CUST00002     2022-01-19    2023-06-27             4              11   \n",
       "4   CUST00002     2022-01-19    2023-06-27             4              11   \n",
       "5   CUST00002     2022-01-19    2023-06-27             4              11   \n",
       "6   CUST00003     2020-07-18    2023-02-06             4              14   \n",
       "7   CUST00003     2020-07-18    2023-02-06             4              14   \n",
       "8   CUST00003     2020-07-18    2023-02-06             4              14   \n",
       "9   CUST00004     2020-03-26    2023-11-28             5              12   \n",
       "\n",
       "   total_revenue  age  gender region    segment  tenure_months  \\\n",
       "0         249000   43  Female  South  Corporate             50   \n",
       "1         249000   43  Female  South  Corporate             50   \n",
       "2         249000   43  Female  South  Corporate             50   \n",
       "3          84300   22    Male   East  Corporate             37   \n",
       "4          84300   22    Male   East  Corporate             37   \n",
       "5          84300   22    Male   East  Corporate             37   \n",
       "6         130150   59  Female  North  Corporate             18   \n",
       "7         130150   59  Female  North  Corporate             18   \n",
       "8         130150   59  Female  North  Corporate             18   \n",
       "9         349000   29    Male   East   Consumer              3   \n",
       "\n",
       "   unique_products  unique_categories  sentiment_mean  sent_share_negative_x  \\\n",
       "0                2                  2            0.00                   0.50   \n",
       "1                2                  2            0.00                   0.50   \n",
       "2                2                  2            0.00                   0.50   \n",
       "3                4                  3            0.75                   0.00   \n",
       "4                4                  3            0.75                   0.00   \n",
       "5                4                  3            0.75                   0.00   \n",
       "6                3                  3            0.25                   0.25   \n",
       "7                3                  3            0.25                   0.25   \n",
       "8                3                  3            0.25                   0.25   \n",
       "9                4                  2            0.60                   0.00   \n",
       "\n",
       "   sent_share_neutral_x  sent_share_positive_x  recency_days       aov  \\\n",
       "0                  0.00                   0.50           166  124500.0   \n",
       "1                  0.00                   0.50           166  124500.0   \n",
       "2                  0.00                   0.50           166  124500.0   \n",
       "3                  0.25                   0.75           187   21075.0   \n",
       "4                  0.25                   0.75           187   21075.0   \n",
       "5                  0.25                   0.75           187   21075.0   \n",
       "6                  0.25                   0.50           328   32537.5   \n",
       "7                  0.25                   0.50           328   32537.5   \n",
       "8                  0.25                   0.50           328   32537.5   \n",
       "9                  0.40                   0.60            33   69800.0   \n",
       "\n",
       "   orders_per_month  rev_90d  rev_90d_share        level_1           0  \\\n",
       "0          0.040000      0.0            0.0    ip_gap_mean   41.000000   \n",
       "1          0.040000      0.0            0.0  ip_gap_median   41.000000   \n",
       "2          0.040000      0.0            0.0     ip_gap_std    0.000000   \n",
       "3          0.108108      0.0            0.0    ip_gap_mean  146.333333   \n",
       "4          0.108108      0.0            0.0  ip_gap_median  138.000000   \n",
       "5          0.108108      0.0            0.0     ip_gap_std  110.735420   \n",
       "6          0.222222      0.0            0.0    ip_gap_mean  339.666667   \n",
       "7          0.222222      0.0            0.0  ip_gap_median  321.000000   \n",
       "8          0.222222      0.0            0.0     ip_gap_std  126.040999   \n",
       "9          1.666667      0.0            0.0    ip_gap_mean  230.000000   \n",
       "\n",
       "   recency  frequency  monetary  recency_scaled  frequency_scaled  \\\n",
       "0      166          2  187000.0       -0.178873         -1.085640   \n",
       "1      166          2  187000.0       -0.178873         -1.085640   \n",
       "2      166          2  187000.0       -0.178873         -1.085640   \n",
       "3      187          4   84300.0        0.023153         -0.035245   \n",
       "4      187          4   84300.0        0.023153         -0.035245   \n",
       "5      187          4   84300.0        0.023153         -0.035245   \n",
       "6      328          4  130150.0        1.379618         -0.035245   \n",
       "7      328          4  130150.0        1.379618         -0.035245   \n",
       "8      328          4  130150.0        1.379618         -0.035245   \n",
       "9       33          5  302000.0       -1.458376          0.489953   \n",
       "\n",
       "   monetary_scaled  cluster segment_label  compound_mean  compound_median  \\\n",
       "0        -0.077402        0     Segment 2       0.285950          0.28595   \n",
       "1        -0.077402        0     Segment 2       0.285950          0.28595   \n",
       "2        -0.077402        0     Segment 2       0.285950          0.28595   \n",
       "3        -0.807727        0     Segment 2       0.051050         -0.00905   \n",
       "4        -0.807727        0     Segment 2       0.051050         -0.00905   \n",
       "5        -0.807727        0     Segment 2       0.051050         -0.00905   \n",
       "6        -0.481676        0     Segment 2       0.276875          0.50615   \n",
       "7        -0.481676        0     Segment 2       0.276875          0.50615   \n",
       "8        -0.481676        0     Segment 2       0.276875          0.50615   \n",
       "9         0.740391        1     Segment 1       0.308720          0.42010   \n",
       "\n",
       "   compound_min  compound_max  neg_mean  neu_mean  pos_mean sentiment_last  \\\n",
       "0        0.0000        0.5719   0.00000   0.67550    0.3245       Positive   \n",
       "1        0.0000        0.5719   0.00000   0.67550    0.3245       Positive   \n",
       "2        0.0000        0.5719   0.00000   0.67550    0.3245       Positive   \n",
       "3       -0.4585        0.6808   0.21400   0.54900    0.2370       Positive   \n",
       "4       -0.4585        0.6808   0.21400   0.54900    0.2370       Positive   \n",
       "5       -0.4585        0.6808   0.21400   0.54900    0.2370       Positive   \n",
       "6       -0.4767        0.5719   0.09575   0.47475    0.4295       Positive   \n",
       "7       -0.4767        0.5719   0.09575   0.47475    0.4295       Positive   \n",
       "8       -0.4767        0.5719   0.09575   0.47475    0.4295       Positive   \n",
       "9       -0.3089        0.5719   0.08580   0.50760    0.4066       Positive   \n",
       "\n",
       "  sentiment_vader_last  compound_last  sent_share_negative_y  \\\n",
       "0             Positive         0.5719                   0.50   \n",
       "1             Positive         0.5719                   0.50   \n",
       "2             Positive         0.5719                   0.50   \n",
       "3             Negative        -0.4585                   0.00   \n",
       "4             Negative        -0.4585                   0.00   \n",
       "5             Negative        -0.4585                   0.00   \n",
       "6             Positive         0.5719                   0.25   \n",
       "7             Positive         0.5719                   0.25   \n",
       "8             Positive         0.5719                   0.25   \n",
       "9             Positive         0.4201                   0.00   \n",
       "\n",
       "   sent_share_neutral_y  sent_share_positive_y  churn  \n",
       "0                  0.00                   0.50      0  \n",
       "1                  0.00                   0.50      0  \n",
       "2                  0.00                   0.50      0  \n",
       "3                  0.25                   0.75      1  \n",
       "4                  0.25                   0.75      1  \n",
       "5                  0.25                   0.75      1  \n",
       "6                  0.25                   0.50      0  \n",
       "7                  0.25                   0.50      0  \n",
       "8                  0.25                   0.50      0  \n",
       "9                  0.40                   0.60      0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def safe_read(path):\n",
    "    try:\n",
    "        return pd.read_csv(path)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# Notebook 2 outputs\n",
    "rfm        = safe_read(\"../data/interim/rfm_features.csv\")            # customer_id, recency, frequency, monetary\n",
    "segments   = safe_read(\"../data/interim/customer_segments.csv\")       # customer_id, cluster\n",
    "\n",
    "# Notebook 5 outputs\n",
    "text_feats = safe_read(\"../data/processed/text_features_customer.csv\")# customer_id, sent shares, VADER stats, last sentiment, churn\n",
    "\n",
    "# Base customer snapshot (from EDA or fallback)\n",
    "base = safe_read(\"../data/processed/customer_features.csv\")\n",
    "if base is None:\n",
    "    raw = safe_read(\"../data/raw/customer_intelligence_dataset.csv\")\n",
    "    if raw is not None:\n",
    "        base = (raw.sort_values([\"customer_id\",\"sale_date\"])\n",
    "                    .groupby(\"customer_id\")\n",
    "                    .agg(churn=(\"churn\",\"max\"),\n",
    "                         age=(\"age\",\"last\"),\n",
    "                         gender=(\"gender\",\"last\"),\n",
    "                         region=(\"region\",\"last\"))\n",
    "                    .reset_index())\n",
    "\n",
    "frames = [df for df in [base, rfm, segments, text_feats] if df is not None]\n",
    "if not frames:\n",
    "    raise RuntimeError(\"No inputs found. Please export from notebooks 1/2/5 first.\")\n",
    "\n",
    "from functools import reduce\n",
    "Xy = reduce(lambda l, r: pd.merge(l, r, on=\"customer_id\", how=\"left\"), frames)\n",
    "\n",
    "# Fix churn in case of merges (churn / churn_x / churn_y)\n",
    "for c in [\"churn\", \"churn_x\", \"churn_y\"]:\n",
    "    if c not in Xy.columns:\n",
    "        Xy[c] = np.nan\n",
    "Xy[\"churn\"] = (Xy[\"churn\"].fillna(Xy[\"churn_x\"])\n",
    "                        .fillna(Xy[\"churn_y\"])\n",
    "                        .astype(float).round().astype(int))\n",
    "Xy = Xy.drop(columns=[c for c in [\"churn_x\",\"churn_y\"] if c in Xy.columns])\n",
    "\n",
    "print(\"Merged shape:\", Xy.shape)\n",
    "print(\"Churn balance:\", Xy[\"churn\"].value_counts(normalize=True).round(3).to_dict())\n",
    "Xy.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric features (17): ['recency', 'frequency', 'monetary', 'compound_mean', 'compound_median', 'compound_min', 'compound_max', 'neg_mean', 'neu_mean', 'pos_mean', 'compound_last', 'sent_share_negative_x', 'sent_share_neutral_x', 'sent_share_positive_x', 'sent_share_negative_y', 'sent_share_neutral_y', 'sent_share_positive_y']\n",
      "One-hot added: ['cluster_0', 'cluster_1']\n",
      "X shape: (5901, 19) | y shape: (5901,)\n"
     ]
    }
   ],
   "source": [
    "# Numeric features from Notebook 2 + Notebook 5\n",
    "rfm_cols   = [\"recency\",\"frequency\",\"monetary\"]\n",
    "text_stats = [\"compound_mean\",\"compound_median\",\"compound_min\",\"compound_max\",\n",
    "              \"neg_mean\",\"neu_mean\",\"pos_mean\",\"compound_last\"]\n",
    "text_shares = [c for c in Xy.columns if c.startswith(\"sent_share_\")]  # e.g., sent_share_negative/neutral/positive\n",
    "\n",
    "num_cols = [c for c in (rfm_cols + text_stats + text_shares) if c in Xy.columns]\n",
    "\n",
    "# Categorical: cluster from Notebook 2\n",
    "cat_cols = []\n",
    "if \"cluster\" in Xy.columns:\n",
    "    cat_cols.append(\"cluster\")\n",
    "\n",
    "# Building X / y\n",
    "X_num = Xy[num_cols].copy().fillna(0.0)\n",
    "\n",
    "if cat_cols:\n",
    "    X_cat = pd.get_dummies(Xy[cat_cols].astype(\"category\"), drop_first=False, prefix=cat_cols)\n",
    "    X = pd.concat([X_num, X_cat], axis=1)\n",
    "else:\n",
    "    X = X_num\n",
    "\n",
    "y = Xy[\"churn\"].astype(int)\n",
    "\n",
    "print(f\"Numeric features ({len(num_cols)}):\", num_cols)\n",
    "if cat_cols:\n",
    "    print(\"One-hot added:\", list(X.columns[len(num_cols):]))\n",
    "print(\"X shape:\", X.shape, \"| y shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (4720, 19) | Test: (1181, 19)\n",
      "y_train pos ratio: 0.25 | y_test pos ratio: 0.25\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, stratify=y, random_state=RSEED\n",
    ")\n",
    "\n",
    "print(\"Train:\", X_train.shape, \"| Test:\", X_test.shape)\n",
    "print(\"y_train pos ratio:\", y_train.mean().round(3), \"| y_test pos ratio:\", y_test.mean().round(3))\n",
    "\n",
    "# Scaling numeric columns for LR/SVM\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = X_train.copy()\n",
    "X_test_scaled  = X_test.copy()\n",
    "X_train_scaled[num_cols] = scaler.fit_transform(X_train[num_cols])\n",
    "X_test_scaled[num_cols]  = scaler.transform(X_test[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before SMOTE: [3539 1181]\n",
      "After  SMOTE (LR/SVM): [3539 3539]\n",
      "After  SMOTE (Trees):  [3539 3539]\n"
     ]
    }
   ],
   "source": [
    "sm = SMOTE(random_state=RSEED, k_neighbors=5)\n",
    "\n",
    "# Resampling for LR/SVM (using scaled features)\n",
    "Xtr_lr_sm,  ytr_lr_sm  = sm.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Resampling for tree models (using unscaled features)\n",
    "Xtr_tree_sm, ytr_tree_sm = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"Before SMOTE:\", np.bincount(y_train))\n",
    "print(\"After  SMOTE (LR/SVM):\", np.bincount(ytr_lr_sm))\n",
    "print(\"After  SMOTE (Trees): \", np.bincount(ytr_tree_sm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Logistic', 'SVM', 'DecisionTree', 'RandomForest']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = {\n",
    "    \"Logistic\": LogisticRegression(max_iter=2000, class_weight=None, random_state=RSEED),\n",
    "    \"SVM\":      SVC(probability=True, class_weight=None, random_state=RSEED),\n",
    "    \"DecisionTree\": DecisionTreeClassifier(max_depth=None, min_samples_leaf=5,\n",
    "                                           random_state=RSEED),\n",
    "    \"RandomForest\": RandomForestClassifier(n_estimators=400, min_samples_leaf=3,\n",
    "                                           max_depth=None, n_jobs=-1, random_state=RSEED),\n",
    "}\n",
    "list(models.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before SMOTE: [3539 1181]\n",
      "After  SMOTE (LR/SVM): [3539 3539]\n",
      "After  SMOTE (Trees):  [3539 3539]\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# SMOTE on the training data only\n",
    "sm = SMOTE(random_state=RSEED, k_neighbors=5)\n",
    "\n",
    "# For LR/SVM, using the *scaled* training features\n",
    "Xtr_lr_sm, ytr_lr_sm = sm.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# For tree models using the *unscaled* training features\n",
    "Xtr_tree_sm, ytr_tree_sm = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"Before SMOTE:\", np.bincount(y_train))\n",
    "print(\"After  SMOTE (LR/SVM):\", np.bincount(ytr_lr_sm))\n",
    "print(\"After  SMOTE (Trees): \", np.bincount(ytr_tree_sm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(name, model, Xtr, ytr, Xte, yte):\n",
    "    model.fit(Xtr, ytr)\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        p = model.predict_proba(Xte)[:,1]\n",
    "    elif hasattr(model, \"decision_function\"):\n",
    "        s = model.decision_function(Xte)\n",
    "        p = (s - s.min()) / (s.max() - s.min() + 1e-9)\n",
    "    else:\n",
    "        p = model.predict(Xte)\n",
    "\n",
    "    y_pred = (p >= 0.5).astype(int)\n",
    "\n",
    "    roc = roc_auc_score(yte, p)\n",
    "    pr  = average_precision_score(yte, p)\n",
    "\n",
    "    print(f\"\\n{name} — ROC-AUC: {roc:.4f} | PR-AUC: {pr:.4f}\")\n",
    "    print(classification_report(yte, y_pred, digits=3))\n",
    "    print(\"Confusion matrix:\\n\", confusion_matrix(yte, y_pred))\n",
    "    return roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic + SMOTE — ROC-AUC: 0.5237 | PR-AUC: 0.2724\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.769     0.514     0.616       886\n",
      "           1      0.268     0.536     0.357       295\n",
      "\n",
      "    accuracy                          0.519      1181\n",
      "   macro avg      0.518     0.525     0.487      1181\n",
      "weighted avg      0.644     0.519     0.551      1181\n",
      "\n",
      "Confusion matrix:\n",
      " [[455 431]\n",
      " [137 158]]\n",
      "\n",
      "SVM + SMOTE — ROC-AUC: 0.6738 | PR-AUC: 0.3779\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.840     0.640     0.726       886\n",
      "           1      0.370     0.634     0.467       295\n",
      "\n",
      "    accuracy                          0.638      1181\n",
      "   macro avg      0.605     0.637     0.597      1181\n",
      "weighted avg      0.722     0.638     0.662      1181\n",
      "\n",
      "Confusion matrix:\n",
      " [[567 319]\n",
      " [108 187]]\n",
      "\n",
      "DecisionTree + SMOTE — ROC-AUC: 0.9174 | PR-AUC: 0.7919\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.897     0.877     0.887       886\n",
      "           1      0.654     0.698     0.675       295\n",
      "\n",
      "    accuracy                          0.832      1181\n",
      "   macro avg      0.776     0.788     0.781      1181\n",
      "weighted avg      0.836     0.832     0.834      1181\n",
      "\n",
      "Confusion matrix:\n",
      " [[777 109]\n",
      " [ 89 206]]\n",
      "\n",
      "RandomForest + SMOTE — ROC-AUC: 0.9772 | PR-AUC: 0.9546\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.941     0.977     0.959       886\n",
      "           1      0.923     0.817     0.867       295\n",
      "\n",
      "    accuracy                          0.937      1181\n",
      "   macro avg      0.932     0.897     0.913      1181\n",
      "weighted avg      0.937     0.937     0.936      1181\n",
      "\n",
      "Confusion matrix:\n",
      " [[866  20]\n",
      " [ 54 241]]\n",
      "\n",
      "SMOTE AUCs: {'Logistic': 0.5237, 'SVM': 0.6738, 'DecisionTree': 0.9174, 'RandomForest': 0.9772}\n",
      "Best (SMOTE): RandomForest | AUC=0.9772\n"
     ]
    }
   ],
   "source": [
    "aucs_sm = {}\n",
    "for name, clf in models.items():\n",
    "    if name in [\"Logistic\", \"SVM\"]:\n",
    "        aucs_sm[name] = eval_model(name + \" + SMOTE\", clf, Xtr_lr_sm, ytr_lr_sm, X_test_scaled, y_test)\n",
    "    else:\n",
    "        aucs_sm[name] = eval_model(name + \" + SMOTE\", clf, Xtr_tree_sm, ytr_tree_sm, X_test, y_test)\n",
    "\n",
    "print(\"\\nSMOTE AUCs:\", {k: round(v,4) for k,v in aucs_sm.items()})\n",
    "best_sm = max(aucs_sm, key=aucs_sm.get)\n",
    "print(f\"Best (SMOTE): {best_sm} | AUC={aucs_sm[best_sm]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic — ROC-AUC: 0.5309 | PR-AUC: 0.2717\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.750     1.000     0.857       886\n",
      "           1      0.000     0.000     0.000       295\n",
      "\n",
      "    accuracy                          0.750      1181\n",
      "   macro avg      0.375     0.500     0.429      1181\n",
      "weighted avg      0.563     0.750     0.643      1181\n",
      "\n",
      "Confusion matrix:\n",
      " [[886   0]\n",
      " [295   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shivensharma/Downloads/Incedo Project/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/shivensharma/Downloads/Incedo Project/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/shivensharma/Downloads/Incedo Project/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SVM — ROC-AUC: 0.7344 | PR-AUC: 0.5253\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.784     0.966     0.866       886\n",
      "           1      0.663     0.200     0.307       295\n",
      "\n",
      "    accuracy                          0.775      1181\n",
      "   macro avg      0.723     0.583     0.586      1181\n",
      "weighted avg      0.754     0.775     0.726      1181\n",
      "\n",
      "Confusion matrix:\n",
      " [[856  30]\n",
      " [236  59]]\n",
      "\n",
      "DecisionTree — ROC-AUC: 0.9168 | PR-AUC: 0.7737\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.901     0.874     0.887       886\n",
      "           1      0.652     0.712     0.681       295\n",
      "\n",
      "    accuracy                          0.833      1181\n",
      "   macro avg      0.777     0.793     0.784      1181\n",
      "weighted avg      0.839     0.833     0.836      1181\n",
      "\n",
      "Confusion matrix:\n",
      " [[774 112]\n",
      " [ 85 210]]\n",
      "\n",
      "RandomForest — ROC-AUC: 0.9825 | PR-AUC: 0.9648\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.906     0.995     0.949       886\n",
      "           1      0.981     0.692     0.811       295\n",
      "\n",
      "    accuracy                          0.920      1181\n",
      "   macro avg      0.944     0.844     0.880      1181\n",
      "weighted avg      0.925     0.920     0.914      1181\n",
      "\n",
      "Confusion matrix:\n",
      " [[882   4]\n",
      " [ 91 204]]\n",
      "\n",
      "BASELINE (no SMOTE) AUCs: {'Logistic': 0.5309, 'SVM': 0.7344, 'DecisionTree': 0.9168, 'RandomForest': 0.9825}\n"
     ]
    }
   ],
   "source": [
    "aucs_base = {}\n",
    "for name, clf in models.items():\n",
    "    if name in [\"Logistic\", \"SVM\"]:\n",
    "        aucs_base[name] = eval_model(name, clf, X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "    else:\n",
    "        aucs_base[name] = eval_model(name, clf, X_train, y_train, X_test, y_test)\n",
    "\n",
    "print(\"\\nBASELINE (no SMOTE) AUCs:\", {k: round(v,4) for k,v in aucs_base.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>monetary</th>\n",
       "      <td>0.129967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recency</th>\n",
       "      <td>0.120874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compound_last</th>\n",
       "      <td>0.075203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compound_mean</th>\n",
       "      <td>0.068112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neu_mean</th>\n",
       "      <td>0.066586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos_mean</th>\n",
       "      <td>0.059013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compound_median</th>\n",
       "      <td>0.055646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compound_min</th>\n",
       "      <td>0.051920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neg_mean</th>\n",
       "      <td>0.050346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compound_max</th>\n",
       "      <td>0.050153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent_share_neutral_y</th>\n",
       "      <td>0.041568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent_share_neutral_x</th>\n",
       "      <td>0.040513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>frequency</th>\n",
       "      <td>0.040353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent_share_negative_x</th>\n",
       "      <td>0.034777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent_share_negative_y</th>\n",
       "      <td>0.033566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent_share_positive_x</th>\n",
       "      <td>0.033022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent_share_positive_y</th>\n",
       "      <td>0.032348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cluster_0</th>\n",
       "      <td>0.008423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cluster_1</th>\n",
       "      <td>0.007609</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       importance\n",
       "monetary                 0.129967\n",
       "recency                  0.120874\n",
       "compound_last            0.075203\n",
       "compound_mean            0.068112\n",
       "neu_mean                 0.066586\n",
       "pos_mean                 0.059013\n",
       "compound_median          0.055646\n",
       "compound_min             0.051920\n",
       "neg_mean                 0.050346\n",
       "compound_max             0.050153\n",
       "sent_share_neutral_y     0.041568\n",
       "sent_share_neutral_x     0.040513\n",
       "frequency                0.040353\n",
       "sent_share_negative_x    0.034777\n",
       "sent_share_negative_y    0.033566\n",
       "sent_share_positive_x    0.033022\n",
       "sent_share_positive_y    0.032348\n",
       "cluster_0                0.008423\n",
       "cluster_1                0.007609"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_name = best_sm.replace(\" + SMOTE\",\"\")\n",
    "best_clf = models[best_name]\n",
    "\n",
    "if best_name in [\"DecisionTree\",\"RandomForest\"]:\n",
    "    if best_name == \"RandomForest\":\n",
    "        best_clf.fit(Xtr_tree_sm, ytr_tree_sm)\n",
    "    else:\n",
    "        best_clf.fit(Xtr_tree_sm, ytr_tree_sm)\n",
    "\n",
    "    importances = pd.Series(best_clf.feature_importances_, index=X.columns)\n",
    "    display(importances.sort_values(ascending=False).head(20).to_frame(\"importance\"))\n",
    "else:\n",
    "    print(\"Best model is not tree-based (no feature_importances_).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../models/churn_model.pkl  (best: RandomForest )\n",
      "Saved: ../models/churn_models_all.pkl  (variants: DecisionTree_BASE, DecisionTree_SMOTE, Logistic_BASE, Logistic_SMOTE, RandomForest_BASE, RandomForest_SMOTE, SVM_BASE, SVM_SMOTE )\n"
     ]
    }
   ],
   "source": [
    "# --- SAVE: best churn model + bundle of ALL variants (BASE & SMOTE) ---\n",
    "import pickle\n",
    "\n",
    "# helpers\n",
    "def _fresh(est):\n",
    "    # new instance with same params (no pipelines)\n",
    "    return est.__class__(**est.get_params())\n",
    "\n",
    "def _fit(est, X, y):\n",
    "    if hasattr(est, \"n_features_in_\"):  # already fitted\n",
    "        return est\n",
    "    est.fit(X, y)\n",
    "    return est\n",
    "\n",
    "# 1) Prepare training matrices for each family/variant\n",
    "X_lr_base,  y_lr_base  = X_train_scaled, y_train         # LR/SVM baseline (scaled)\n",
    "X_lr_sm,    y_lr_sm    = Xtr_lr_sm, ytr_lr_sm            # LR/SVM + SMOTE (scaled)\n",
    "\n",
    "X_tree_base,y_tree_base= X_train, y_train                # Trees baseline (unscaled)\n",
    "X_tree_sm,  y_tree_sm  = Xtr_tree_sm, ytr_tree_sm        # Trees + SMOTE (unscaled)\n",
    "\n",
    "# 2) Collect your estimators from the `models` dict (with simple fallbacks)\n",
    "algs = {}\n",
    "if \"models\" in globals() and isinstance(models, dict):\n",
    "    algs.update(models)\n",
    "else:\n",
    "    if \"lr\" in globals(): algs[\"Logistic\"] = lr\n",
    "    if \"svm\" in globals(): algs[\"SVM\"] = svm\n",
    "    if \"dt\" in globals(): algs[\"DecisionTree\"] = dt\n",
    "    if \"rf\" in globals(): algs[\"RandomForest\"] = rf\n",
    "\n",
    "# 3) Build all fitted variants\n",
    "all_variants = {}\n",
    "\n",
    "# LR & SVM (scaled)\n",
    "for key in [\"Logistic\", \"SVM\"]:\n",
    "    if key in algs:\n",
    "        m_base = _fresh(algs[key])\n",
    "        m_sm   = _fresh(algs[key])\n",
    "        all_variants[f\"{key}_BASE\"]  = {\"model\": _fit(m_base, X_lr_base, y_lr_base),\n",
    "                                        \"uses_scaler\": True,  \"variant\": \"BASE\"}\n",
    "        all_variants[f\"{key}_SMOTE\"] = {\"model\": _fit(m_sm,   X_lr_sm,   y_lr_sm),\n",
    "                                        \"uses_scaler\": True,  \"variant\": \"SMOTE\"}\n",
    "\n",
    "# Trees (unscaled)\n",
    "for key in [\"DecisionTree\", \"RandomForest\"]:\n",
    "    if key in algs:\n",
    "        m_base = _fresh(algs[key])\n",
    "        m_sm   = _fresh(algs[key])\n",
    "        all_variants[f\"{key}_BASE\"]  = {\"model\": _fit(m_base, X_tree_base, y_tree_base),\n",
    "                                        \"uses_scaler\": False, \"variant\": \"BASE\"}\n",
    "        all_variants[f\"{key}_SMOTE\"] = {\"model\": _fit(m_sm,   X_tree_sm,   y_tree_sm),\n",
    "                                        \"uses_scaler\": False, \"variant\": \"SMOTE\"}\n",
    "\n",
    "# 4) Best model artifact (same structure you already use)\n",
    "best_name_core = best_sm.replace(\" + SMOTE\",\"\")\n",
    "best_uses_scaler = (\"Logistic\" in best_sm) or (\"SVM\" in best_sm)\n",
    "best_key = f\"{best_name_core}_{'SMOTE' if 'SMOTE' in best_sm else 'BASE'}\"\n",
    "\n",
    "if best_key not in all_variants:\n",
    "    raise RuntimeError(f\"Best model '{best_key}' not found among fitted variants.\")\n",
    "\n",
    "best_fitted = all_variants[best_key][\"model\"]\n",
    "\n",
    "best_artifacts = {\n",
    "    \"best_model_name\": best_sm,\n",
    "    \"feature_columns\": list(X.columns),\n",
    "    \"numeric_columns\": num_cols,\n",
    "    \"scaler\": scaler if best_uses_scaler else None,\n",
    "    \"model\": best_fitted\n",
    "}\n",
    "\n",
    "with open(\"../models/churn_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(best_artifacts, f)\n",
    "print(\"Saved: ../models/churn_model.pkl  (best:\", best_sm, \")\")\n",
    "\n",
    "# 5) Save ALL variants bundle (with schema once; scaler included for convenience)\n",
    "all_bundle = {\n",
    "    \"feature_columns\": list(X.columns),\n",
    "    \"numeric_columns\": num_cols,\n",
    "    \"scaler\": scaler,                    # only used when 'uses_scaler' == True\n",
    "    \"models\": all_variants               # name -> {model, uses_scaler, variant}\n",
    "}\n",
    "\n",
    "with open(\"../models/churn_models_all.pkl\", \"wb\") as f:\n",
    "    pickle.dump(all_bundle, f)\n",
    "print(\"Saved: ../models/churn_models_all.pkl  (variants:\", \", \".join(sorted(all_variants.keys())), \")\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
